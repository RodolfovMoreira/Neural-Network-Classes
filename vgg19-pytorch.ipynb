{"cells":[{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-one\"></a>\n# Introduction\n\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import time\nfrom PIL import Image\nimport os\nimport cv2\nimport numpy as np\nimport torch\nimport torch.optim as optim\nfrom torch import nn\nfrom torch.autograd import Variable\nfrom collections import OrderedDict\nimport torchvision\nimport torchvision.models as models\nimport torch.utils.model_zoo as model_zoo\nimport torchvision.transforms as transforms\nfrom torchvision import datasets\n\nfrom torch.utils.data import Dataset,DataLoader\nfrom itertools import accumulate\nfrom functools import reduce\n\n\nfrom glob import glob\nfrom sklearn.model_selection import GroupKFold, StratifiedKFold\nimport cv2\nfrom skimage import io\nimport torch\nfrom torch import nn\nimport os\nfrom datetime import datetime\nimport time\nimport random\nimport cv2\nimport torchvision\nfrom torchvision import transforms\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\n\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import Dataset,DataLoader\nfrom torch.utils.data.sampler import SequentialSampler, RandomSampler\nfrom torch.cuda.amp import autocast, GradScaler\nfrom torch.nn.modules.loss import _WeightedLoss\nimport torch.nn.functional as F\n\n# import timm\n\nimport sklearn\nimport warnings\nimport joblib\nfrom sklearn.metrics import roc_auc_score, log_loss\nfrom sklearn import metrics\nimport warnings\nimport cv2\nimport json","execution_count":1,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"train_data = pd.read_csv('../input/cassava-leaf-disease-classification/train.csv')\nprint('Total nuber of images in the dataset', len(train_data))\ntrain_data.head()","execution_count":2,"outputs":[{"output_type":"stream","text":"Total nuber of images in the dataset 21397\n","name":"stdout"},{"output_type":"execute_result","execution_count":2,"data":{"text/plain":"         image_id  label\n0  1000015157.jpg      0\n1  1000201771.jpg      3\n2   100042118.jpg      1\n3  1000723321.jpg      1\n4  1000812911.jpg      3","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_id</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1000015157.jpg</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1000201771.jpg</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>100042118.jpg</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1000723321.jpg</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1000812911.jpg</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"with open('../input/cassava-leaf-disease-classification/label_num_to_disease_map.json') as f:\n    labels = json.load(f)\nlabels","execution_count":3,"outputs":[{"output_type":"execute_result","execution_count":3,"data":{"text/plain":"{'0': 'Cassava Bacterial Blight (CBB)',\n '1': 'Cassava Brown Streak Disease (CBSD)',\n '2': 'Cassava Green Mottle (CGM)',\n '3': 'Cassava Mosaic Disease (CMD)',\n '4': 'Healthy'}"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = train_data['label'].astype(str).map(labels)\np = plt.hist(data)\nplt.xticks(rotation='vertical')\n\nplt.show()","execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-two\"></a>\n# Transfer learning \n\nTransfer learning always helps as a pre-trained model had already learnt the high level features. Selecting the VGG-19 model for initial modelling and visualization\n"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-oneintwo\"></a>\n## Loading the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_urls = {\n    'vgg19': 'https://download.pytorch.org/models/vgg19-dcbb9e9d.pth',    \n}\n\nmodel_names = model_urls.keys()\n\ninput_sizes = {\n    'vgg' : (224,224)\n}\n\nbatch_size = 20\nuse_gpu = torch.cuda.is_available()","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Sanity check that param names overlap\n#Note that params are not necessarily in the same order for every pretrained model\ndef diff_states(dict_canonical, dict_subset):\n    names1, names2 = (list(dict_canonical.keys()), list(dict_subset.keys()))\n    not_in_1 = [n for n in names1 if n not in names2]\n    not_in_2 = [n for n in names2 if n not in names1]\n    assert len(not_in_1) == 0\n    assert len(not_in_2) == 0\n\n    for name, v1 in dict_canonical.items():\n        v2 = dict_subset[name]\n        assert hasattr(v2, 'size')\n        if v1.size() != v2.size():\n            yield (name, v1)          ","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#loading and finding the different states in the model and merging them\ndef load_defined_model(name, num_classes):\n    \n    model = models.__dict__[name](num_classes=num_classes)\n    \n    pretrained_state = model_zoo.load_url(model_urls[name])\n\n    #Diff\n    diff = [s for s in diff_states(model.state_dict(), pretrained_state)]\n    print(\"Replacing the following state from initialized\", name, \":\", \\\n          [d[0] for d in diff])\n    \n    for name, value in diff:\n        pretrained_state[name] = value\n    \n    assert len([s for s in diff_states(model.state_dict(), pretrained_state)]) == 0\n    \n    #Merge\n    model.load_state_dict(pretrained_state)\n    return model, diff","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def prepare_dataloader(df, trn_idx, val_idx, data_root='../input/cassava-leaf-disease-classification/train_images/'):\n    \n    from catalyst.data.sampler import BalanceClassSampler\n    \n    train_ = df.loc[trn_idx,:].reset_index(drop=True)\n    valid_ = df.loc[val_idx,:].reset_index(drop=True)\n        \n    train_ds = CassavaDataset(train_, data_root, transforms=get_train_transforms(), output_label=True, one_hot_label=False, do_fmix=False, do_cutmix=False)\n    valid_ds = CassavaDataset(valid_, data_root, transforms=get_valid_transforms(), output_label=True)\n    \n    train_loader = torch.utils.data.DataLoader(\n        train_ds,\n        batch_size=20,\n        pin_memory=False,\n        drop_last=False,\n        shuffle=True\n        #sampler=BalanceClassSampler(labels=train_['label'].values, mode=\"downsampling\")\n    )\n    val_loader = torch.utils.data.DataLoader(\n        valid_ds, \n        batch_size=20,\n        shuffle=False,\n        pin_memory=False,\n    )\n    return train_loader, val_loader","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def filtered_params(net, param_list=None):\n    def in_param_list(s):\n        for p in param_list:\n            if s.endswith(p):\n                return True\n        return False    \n    #Caution: DataParallel prefixes '.module' to every parameter name\n    params = net.named_parameters() if param_list is None \\\n    else (p for p in net.named_parameters() if in_param_list(p[0]))\n    return params","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef rand_bbox(size, lam):\n    W = size[0]\n    H = size[1]\n    cut_rat = np.sqrt(1. - lam)\n    cut_w = np.int(W * cut_rat)\n    cut_h = np.int(H * cut_rat)\n\n    # uniform\n    cx = np.random.randint(W)\n    cy = np.random.randint(H)\n\n    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n    bby1 = np.clip(cy - cut_h // 2, 0, H)\n    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n    bby2 = np.clip(cy + cut_h // 2, 0, H)\n    return bbx1, bby1, bbx2, bby2\n\n\nclass CassavaDataset(Dataset):\n    def __init__(self, df, data_root, \n                 transforms=None, \n                 output_label=True, \n                 one_hot_label=False,\n                 do_fmix=False, \n                 fmix_params={\n                     'alpha': 1., \n                     'decay_power': 3., \n                     'shape': (224,224),\n                     'max_soft': True, \n                     'reformulate': False\n                 },\n                 do_cutmix=False,\n                 cutmix_params={\n                     'alpha': 1,\n                 }\n                ):\n        \n        super().__init__()\n        self.df = df.reset_index(drop=True).copy()\n        self.transforms = transforms\n        self.data_root = data_root\n        self.do_fmix = do_fmix\n        self.fmix_params = fmix_params\n        self.do_cutmix = do_cutmix\n        self.cutmix_params = cutmix_params\n        \n        self.output_label = output_label\n        self.one_hot_label = one_hot_label\n        \n        if output_label == True:\n            self.labels = self.df['label'].values\n            #print(self.labels)\n            \n            if one_hot_label is True:\n                self.labels = np.eye(self.df['label'].max()+1)[self.labels]\n                #print(self.labels)\n            \n    def __len__(self):\n        return self.df.shape[0]\n    \n    def __getitem__(self, index: int):\n        \n        # get labels\n        if self.output_label:\n            target = self.labels[index]\n          \n        img  = get_img(\"{}/{}\".format(self.data_root, self.df.loc[index]['image_id']))\n\n        if self.transforms:\n            img = self.transforms(image=img)['image']\n        \n        if self.do_fmix and np.random.uniform(0., 1., size=1)[0] > 0.5:\n            with torch.no_grad():\n                #lam, mask = sample_mask(**self.fmix_params)\n                \n                lam = np.clip(np.random.beta(self.fmix_params['alpha'], self.fmix_params['alpha']),0.6,0.7)\n                \n                # Make mask, get mean / std\n                mask = make_low_freq_image(self.fmix_params['decay_power'], self.fmix_params['shape'])\n                mask = binarise_mask(mask, lam, self.fmix_params['shape'], self.fmix_params['max_soft'])\n    \n                fmix_ix = np.random.choice(self.df.index, size=1)[0]\n                fmix_img  = get_img(\"{}/{}\".format(self.data_root, self.df.iloc[fmix_ix]['image_id']))\n\n                if self.transforms:\n                    fmix_img = self.transforms(image=fmix_img)['image']\n\n                mask_torch = torch.from_numpy(mask)\n                \n                # mix image\n                img = mask_torch*img+(1.-mask_torch)*fmix_img\n\n                #print(mask.shape)\n\n                #assert self.output_label==True and self.one_hot_label==True\n\n                # mix target\n                rate = mask.sum()/224/224\n                target = rate*target + (1.-rate)*self.labels[fmix_ix]\n                #print(target, mask, img)\n                #assert False\n        \n        if self.do_cutmix and np.random.uniform(0., 1., size=1)[0] > 0.5:\n            #print(img.sum(), img.shape)\n            with torch.no_grad():\n                cmix_ix = np.random.choice(self.df.index, size=1)[0]\n                cmix_img  = get_img(\"{}/{}\".format(self.data_root, self.df.iloc[cmix_ix]['image_id']))\n                if self.transforms:\n                    cmix_img = self.transforms(image=cmix_img)['image']\n                    \n                lam = np.clip(np.random.beta(self.cutmix_params['alpha'], self.cutmix_params['alpha']),0.3,0.4)\n                bbx1, bby1, bbx2, bby2 = rand_bbox((224, 224), lam)\n\n                img[:, bbx1:bbx2, bby1:bby2] = cmix_img[:, bbx1:bbx2, bby1:bby2]\n\n                rate = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (224* 224))\n                target = rate*target + (1.-rate)*self.labels[cmix_ix]\n                \n            #print('-', img.sum())\n            #print(target)\n            #assert False\n                            \n        # do label smoothing\n        #print(type(img), type(target))\n        if self.output_label == True:\n            return img, target\n        else:\n            return img","execution_count":10,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-twointwo\"></a>\n## Training and Validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Data augmentation\nfrom albumentations.augmentations.transforms import CLAHE\nfrom albumentations import (\n    HorizontalFlip, VerticalFlip, IAAPerspective, ShiftScaleRotate, CLAHE, RandomRotate90,\n    Transpose, ShiftScaleRotate, Blur, OpticalDistortion, GridDistortion, HueSaturationValue,\n    IAAAdditiveGaussianNoise, GaussNoise, MotionBlur, MedianBlur, IAAPiecewiseAffine, RandomResizedCrop,\n    IAASharpen, IAAEmboss, RandomBrightnessContrast, Flip, OneOf, Compose, Normalize, Cutout, CoarseDropout, ShiftScaleRotate, CenterCrop, Resize\n)\n\nfrom albumentations.pytorch import ToTensorV2\n\ndef get_train_transforms():\n    return Compose([\n            RandomResizedCrop(224, 224),\n            Transpose(p=0.5),\n            HorizontalFlip(p=0.5),\n            VerticalFlip(p=0.5),\n            ShiftScaleRotate(p=0.5),\n            HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.5),\n            RandomBrightnessContrast(brightness_limit=(-0.1,0.1), contrast_limit=(-0.1, 0.1), p=0.5),\n            CLAHE(clip_limit=4.0, tile_grid_size=(8, 8), always_apply=True, p=1.0),\n            Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n            CoarseDropout(p=0.5),\n            Cutout(p=0.5),\n            ToTensorV2(p=1.0),\n        ], p=1.)\n  \n        \ndef get_valid_transforms():\n    return Compose([\n            CenterCrop(224, 224, p=1.),\n            Resize(224,224),\n            CLAHE(clip_limit=4.0, tile_grid_size=(8, 8), always_apply=True, p=1.0),\n            Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n            ToTensorV2(p=1.0),\n        ], p=1.)","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_one_epoch(epoch, model, loss_fn, optimizer, train_loader, device, scheduler=None, schd_batch_update=False):\n    model.train()\n\n    t = time.time()\n    running_loss = None\n\n    pbar = tqdm(enumerate(train_loader), total=len(train_loader))\n    for step, (imgs, image_labels) in pbar:\n        imgs = imgs.to(device).float()\n        image_labels = image_labels.to(device).long()\n\n        #print(image_labels.shape, exam_label.shape)\n        \n        image_preds = model(imgs)   #output = model(input)\n        #print(image_preds.shape, exam_pred.shape)\n\n        loss = loss_fn(image_preds, image_labels)\n            \n        loss.backward()\n\n        if running_loss is None:\n            running_loss = loss.item()\n        else:\n            running_loss = running_loss * .99 + loss.item() * .01\n\n        if ((step + 1) %  2 == 0) or ((step + 1) == len(train_loader)):\n                # may unscale_ here if desired (e.g., to allow clipping unscaled gradients)\n\n            optimizer.step()\n            optimizer.zero_grad() \n                \n            if scheduler is not None and schd_batch_update:\n                scheduler.step()\n\n        if ((step + 1) % 1 == 0) or ((step + 1) == len(train_loader)):\n            description = f'epoch {epoch} loss: {running_loss:.4f}'\n                \n            pbar.set_description(description)\n                \n    if scheduler is not None and not schd_batch_update:\n        scheduler.step()\n        \ndef valid_one_epoch(epoch, model, loss_fn, val_loader, device, scheduler=None, schd_loss_update=False):\n    model.eval()\n\n    t = time.time()\n    loss_sum = 0\n    sample_num = 0\n    image_preds_all = []\n    image_targets_all = []\n    \n    pbar = tqdm(enumerate(val_loader), total=len(val_loader))\n    for step, (imgs, image_labels) in pbar:\n        imgs = imgs.to(device).float()\n        image_labels = image_labels.to(device).long()\n        \n        image_preds = model(imgs)   #output = model(input)\n        #print(image_preds.shape, exam_pred.shape)\n        image_preds_all += [torch.argmax(image_preds, 1).detach().cpu().numpy()]\n        image_targets_all += [image_labels.detach().cpu().numpy()]\n        \n        loss = loss_fn(image_preds, image_labels)\n        \n        loss_sum += loss.item()*image_labels.shape[0]\n        sample_num += image_labels.shape[0]  \n\n        if ((step + 1) % 1 == 0) or ((step + 1) == len(val_loader)):\n            description = f'epoch {epoch} loss: {loss_sum/sample_num:.4f}'\n            pbar.set_description(description)\n    \n    image_preds_all = np.concatenate(image_preds_all)\n    image_targets_all = np.concatenate(image_targets_all)\n    print('validation multi-class accuracy = {:.4f}'.format((image_preds_all==image_targets_all).mean()))\n    \n    if scheduler is not None:\n        if schd_loss_update:\n            scheduler.step(loss_sum/sample_num)\n        else:\n            scheduler.step()","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_img(path):\n    im_bgr = cv2.imread(path)\n    im_rgb = im_bgr[:, :, ::-1]\n    #print(im_rgb)\n    return im_rgb","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"seed_everything(719)\n    \nfolds = StratifiedKFold(n_splits=3, shuffle=True, random_state=719).split(np.arange(train_data.shape[0]), train_data.label.values)\n\nfor fold, (trn_idx, val_idx) in enumerate(folds):\n# we'll train fold 0 first\n    if fold > 0:\n        break \n\n    print('Training with {} started'.format(fold))\n\n    print(len(trn_idx), len(val_idx))\n    train_loader, val_loader = prepare_dataloader(train_data, trn_idx, val_idx, data_root='../input/cassava-leaf-disease-classification/train_images/')\n\n    device = torch.device('cuda:0')\n\n    model, diff = load_defined_model('vgg19', 5)\n    model =model.cuda()\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-6)\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=1, eta_min=1e-6, last_epoch=-1)\n                                            \n\n    loss_tr = nn.CrossEntropyLoss().to('cuda:0') \n    loss_fn = nn.CrossEntropyLoss().to('cuda:0')\n\n    for epoch in range(1):\n        train_one_epoch(epoch, model, loss_tr, optimizer, train_loader, device, scheduler=scheduler, schd_batch_update=False)\n\n        with torch.no_grad():\n            valid_one_epoch(epoch, model, loss_fn, val_loader, device, scheduler=None, schd_loss_update=False)\n\n        torch.save(model.state_dict(),'vgg19_epoch_20')\n\n    #torch.save(model.cnn_model.state_dict(),'{}/cnn_model_fold_{}_{}'.format(CFG['model_path'], fold, CFG['tag']))\n    del model, optimizer, train_loader, val_loader, scheduler\n    torch.cuda.empty_cache()","execution_count":15,"outputs":[{"output_type":"stream","text":"Training with 0 started\n14264 7133\n","name":"stdout"},{"output_type":"stream","text":"Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n","name":"stderr"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=574673361.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"61b8b07127a544b8a7d0f3abbc17987b"}},"metadata":{}},{"output_type":"stream","text":"\nReplacing the following state from initialized vgg19 : ['classifier.6.weight', 'classifier.6.bias']\n","name":"stdout"},{"output_type":"stream","text":"epoch 0 loss: 0.7695: 100%|██████████| 714/714 [07:16<00:00,  1.64it/s]\nepoch 0 loss: 0.7355: 100%|██████████| 357/357 [02:26<00:00,  2.44it/s]\n","name":"stderr"},{"output_type":"stream","text":"validation multi-class accuracy = 0.7349\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"What is an healthy cassava plant, if you could extract the veins and leaf margins from the images evidently, it is healthy."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-four\"></a>\n# Conclusion\n\n\n1. We understood the symptoms of each diseases with respect to the labels in the dataset\n2. Through Transfer learning, and training we dealth with developing a deep learning model for Cassava Plant Disease Classification\n3. We also visualized the model performance and checked whether it matches the data analysis.\n4. Dealing with the label noise is our next step."},{"metadata":{},"cell_type":"markdown","source":"-----"}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}